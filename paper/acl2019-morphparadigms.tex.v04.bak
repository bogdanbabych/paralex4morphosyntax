%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage[T2A,T1]{fontenc}
\usepackage[ukrainian,english]{babel}
\usepackage{graphicx}


\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Unsupervised induction of Ukrainian morphological paradigms for the new lexicon: extending coverage for Named Entities and neologisms using inflection tables and unannotated corpora.}
%\title{Instructions for ACL 2019 Proceedings}


\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
	The paper presents an unsupervised method for quickly extending a Ukrainian lexicon by generating paradigms and morphological feature structures for new Named Entities and neologisms, which are not covered by existing static morphological resources. This approach addresses a practical problem of modelling paradigms for entities created by the dynamic processes in the lexicon: this problem is especially serious for highly-inflected languages in  domains with specialised or quickly changing lexicon. The method uses an unannotated Ukrainian corpus and a small fixed set of inflection tables, which can be found in traditional grammar textbooks. The advantage of the proposed approach is that updating the morphological lexicon does not require training or linguistic annotation, allowing fast knowledge-light extension of an existing static lexicon to improve morphological coverage on a specific corpus. The method is implemented in an open-source package on an (anonymous) GitHub repository \url{https://github.com/qumtie/paralex4morphosyntax}. It can be applied to other low-resourced inflectional languages which have internet corpora and linguistic descriptions of their inflection system, following the example of inflection tables for Ukrainian. Evaluation results shows consistent improvements in coverage for Ukrainian corpora of different genres.
	

\end{abstract}

%\begin{abstract}
%  This document contains the instructions for preparing a camera-ready
%  manuscript for the proceedings of ACL 2019. The document itself
%  conforms to its own specifications, and is therefore an example of
%  what your manuscript should look like. These instructions should be
%  used for both papers submitted for review and for final versions of
%  accepted papers.  Authors are asked to conform to all the directions
%  reported in this document.
%\end{abstract}

\section{Introduction}


\begin{quote}
	"Our language can be regarded as an ancient city: a maze of little streets and squares, of old and new houses, of houses with extensions from various periods, and all this surrounded by a multitude of new suburbs with straight and regular streets and uniform houses." \cite{Wittgenstein-2009}
\end{quote}
This metaphor from Wittgenstein's later work `Philosophical Investigations' may be applied to two aspects of the natural language lexicon, which so far have received little attention in computational linguistics. Firstly, like a city, the lexicon constantly evolves, reflecting political and technical changes in the society that take place very rapidly, so it may be insufficient to design static lexical resources and to expect that they would give the same high level of corpus coverage once and for all: the lexicon needs to be constantly updated to reflect live changes in the system. Secondly, even though there may be many irregularities in the lexicon, similar to `a maze of little streets', this more often happens with an older lexical core, while new words typically follow more `straight and regular' patterns, so the task of updating the lexicon for natural language applications may be facilitated by this tendency.

This paper investigates the extent of the new lexicon problem for different types of Ukrainian corpora and further proposes and evaluates a knowledge-light approach to extending lexical coverage of morphological resources to neologisms (new words, meanings or usages) and new Named Entities which follow regular inflectional patterns.

Morphological annotation of the lexicon is an important component for many natural language processing pipelines, such as part-of-speech tagging, morphological disambiguation, parsing, semantic analysis, as well as for applications such as machine translation, information extraction, terminology detection, etc. For example, in part-of-speech tagging the morphological lexicon normally supplies lemmas and associated sets of possible parts-of-speech and values of morphological categories for each token (e.g., values for grammatical case: nominative, genitive, dative, accusative, instrumental, locative, vocative; number: singular, plural; gender: masculine, feminine, neuter; person: 1st, 2nd, 3rd; mood: indicative, imperative, subjunctive; tense: past, present, future, etc.), which in case of potential ambiguity the tagger resolves using tables of transition probabilities, trained neural networks, etc. For any language the creation of a morphological lexicon is difficult, because of a large number of lexical types needed to achieve good corpus coverage and also because of irregularities in word paradigms (systems of inflected word forms, lemmas and associated morphological features). For highly inflected Slavonic languages creation of the morphological lexicon is even more challenging, since most words have complex morphological paradigms, which require fine-grained annotation of parts-of-speech and their grammatical sub-categories. Creation of high-quality morphological resources for these languages often requires an extensive effort over many years.

Like the majority of other Slavonic languages, Ukrainian is a highly inflected language with the `synthetic' grammar structure (where grammatical relations are predominantly marked within content word forms), so the task of morphological paradigm generation for it is not trivial. It is also more critical for the accuracy of related tasks, such as part-of-speech tagging, because of a larger potential number of morphological value combinations: it is harder to guess the correct part-of-speech tag based on neighbouring tags in the case of a missing word form. Ukrainian paradigms for inflected parts of speech have between 7 and 28 distinct morphological feature combinations and associated word forms for a single lemma, and there is both regular and irregular ambiguity within and across different parts-of-speech and lexicogrammatical classes of words (i.e., animate vs. inanimate nouns, perfective vs. imperfective verbs). In Ukrainian, as in other highly-inflected languages most morphological information is supplied within the word rather than by the context, so lexical gaps are more detrimental to the performance and correct prediction of inflected word forms and their morphological characteristics becomes essential.

For Ukrainian there exist wide-coverage static lexical resources (see Section~\ref{sec:PrevWork}), however, extending them in a traditional way would involve continuous annotation effort requiring linguistic expertise and near-native knowledge of the language, making it hard to keep up with the most recent lexical developments. 

The approach proposed in this paper is designed for the scenario where for a highly-inflected language there exists a hand-crafted static morphological lexicon that covers potentially irregular and more frequent lexical core. For extending this lexicon to cover new regularly inflected entities we use an internet corpus and small inflection tables from grammar textbooks, e.g., \cite{hryshchenko1997sulm}, \cite{press2015ukrainian}: such resources would often be available for other low-resourced languages, since the tasks that would require linguistic expertise -- creating the core lexicon and inflection tables -- need to be done only once, so paradigms for new entities can be automatically created whenever a new corpus becomes available. Core static morphological lexicons have been developed for several low-resourced languages as stand-alone resources and within shared frameworks, such as Universal Dependencies \cite{nivre2016universal}, Apertium \cite{forcada2011apertium} (specifically in the context of Machine Translation) or Grammatical Framework \cite{ranta11gram} (in limited subject domains). However, the task of extending morphological lexicon in response to dynamic processes in the lexical system, emergence neologisms, new terminology or Named Entities so far has not been systematically addressed.

The paper is organised as follows. In Section~\ref{sec:PrevWork} we review some of the previous work in the area, in Section~\ref{sec:Algorithm} we describe the algorithm, datasets and an experiment on generating paradigms, in Section~\ref{sec:Evaluation} we present experimental results on comparative evaluation of lexical coverage on different corpora for the baseline static morphological lexicon and for the extended paradigms which cover the new lexicon. 

\section{Previous work}
\label{sec:PrevWork}

Several projects have addressed the problem of developing the Ukrainian morphological lexicon and morphological disambiguation strategies:~\cite{gryaznukhina-ed-1989}  \cite{Rysin-Starko-2019}, \cite{kotsyba2009ugtag}, \cite{kotsyba2010multext}, \cite{babych2016ukrainian}. For the experiments presented in this paper we use the most complete morphological lexicon from \cite{Rysin-Starko-2019}, which in its current implementation contains a wide-coverage lexicon: 366,846 Ukrainian lemmas which are expanded into 5,690,688 word forms with corresponding morphological feature combinations. 

We evaluate the coverage of this lexicon on large Ukrainian corpora collected in lang-uk project \cite{Dyomkin-2019}, Table~\ref{lang-uk-corpus-description} is taken from this source, which describes these collections.


\begin{table}[]
	\begin{center}
		\begin{tabular}{|l|rr|}
			\hline \textbf{Corpus} & \textbf{No of words} & \textbf{No of sent} \\ \hline
			News & 461,451,019 & 31,021,650 \\
			Wikipedia & 185,645,357 & 15,786,948 \\
			Fiction & 18,323,509 & 1,811,548 \\
			Laws and legal acts & 578,988,264 & 29,208,302 \\
			\hline
			Total & 1,244,408,149 &  77,828,448\\
			\hline
		\end{tabular}
	\end{center}
	\caption{\label{lang-uk-corpus-description} Description of Ukrainian corpora. }
\end{table}


Detailed overviews of different approaches to developing morphological lexicons can be found in \cite{ahlberg2015paradigm}, \cite{koskenniemi2018guessing} and  \cite{fam2018ips}. For our purposes existing approaches can be characterised by their application scenarios and assumptions about available datasets. Interesting work has been done in neural, supervised and semi-supervised frameworks, e.g., \cite{ahlberg2015paradigm}, \cite{hulden2014semi}, \cite{koskenniemi2018guessing}, \cite{silfverberg2018computational}, \cite{wolf2018structured}, \cite{kirov2018recurrent}, \cite{faruqui2016morpho}, \cite{faruqui2015morphological}, \cite{aharoni2016morphological}, \cite{cotterell2017paradigm}. Much of this work assumes availability of partially labeled data, such as word paradigms and/or clean datasets, such as lists of `headwords' (lemmas) from which paradigms are generated. \cite{fam2018ips} identify three main approaches to learning morphological inflection: the hand-engineered rule-based approach, which requires much cost and time for construction, the supervised approach, which relies on initial labelled datasets and the neural approach, which needs more training time and even more data. However, for low-resource scenarios more attention need to be given to unsupervised knowledge-light methods, which could make strong assumptions, e.g., based on compact  linguistic descriptions of the inflection systems, but for the most part rely only unlabelled data or resources that would be typically available for low-resource languages.

A terminological note: in several papers, such as \cite{ahlberg2015paradigm}, \cite{silfverberg2018computational}, the term `paradigm' is used to describe a generalised inflection pattern, which could apply to a class of words, while the term `inflection table' characterises an individual system of inflection for a single word. This usage differs from the traditional understanding of the notion of a paradigm, as a system of word forms for a given word, as understood in \cite{spencer2001paradigm}. In our paper we adhere to the traditional terminological usage for the term `paradigm' as a system of word forms, and refer to `inflection tables' as tables of inflection only, which may be attached to a class of stems.

The problem of characterising dynamic processes in the Ukrainian lexicon has been discussed in \cite{klymenko2008dynamichni}, \cite{karpilovska2013aktyvni}, where these changes are attributed to political, cultural and technical developments in the society -- the active `social dynamics', which causes the active `linguistic dynamics': renewal and additions to nominative and communicative resources of the language and changes in linguistic norms. While the grammar or phonology remain more conservative, the lexicon is very open to such changes. There is an ongoing work to record such lexical developments for Ukrainian and other languages, however, so far there is no framework for modelling morphological features and inflections for neologisms and new Named Entities.

\section{Algorithm description}
\label{sec:Algorithm}

The proposed algorithm uses small inflection tables and unannotated corpus (or a frequency list compiled from such a corpus). It attempts to split each token in the corpus into its stem and inflection using all inflections in all available inflection tables. When a split is successful, it generates a full hypothetical paradigm consistent with the split, using the identified stem and all other inflections for the given table. Then these hypotheses are checked against available word forms in the corpus: whether a sufficient number of forms can be found to confirm the hypothetical paradigm. In this approach for the paradigms to be generated reliably, the new entities need to have a sufficient morphological diversity in the corpus: we experimentally established that at least 3 or 4 different word forms are needed to make a sufficiently accurate prediction of a paradigm and its remaining unseen word forms. For confirmed paradigms the algorithm generates all remaining word forms, their lemma (as a designated `dictionary' word form in the paradigm, e.g., a nominative singular form of a noun) and sets of morphological category values associated with inflections, based on the expected structure of the paradigm. Multiple splits of a token are possible, so hypothetical paradigms are ranked by the number of confirmed word forms, and the paradigm with the highest number is selected among the competing paradigms. Figure \ref{fig:alg1} shows the general overview of the algorithm.

\begin{figure}
	\includegraphics[width=\linewidth]{algorithm-description.jpg}
	\caption{Algorithm description.}
	\label{fig:alg1}
\end{figure}

For a general case to cover less regular paradigms with stem alternations the algorithm may be complemented with a distortion model which modifies tested tokens and hypothetical word forms according to morphonological rules of the language, for Ukrainian this would cover historical alternations such as [o,e] -> [i] in `newly closed' syllables, e.g., [kon'a] (horse.Gen.sing) -> [kin'] (horse.Nom.sing), [h, k, x] before [i] -> [z, ts, s], e.g., [ruka] (hand.Nom.sing) -> [rutsi] (hand.Dat.sing), etc.  

The example in Figure~\ref{fig:alg_example} illustrates working of the algorithm. In this example we assume that the current token is \selectlanguage{ukrainian}рука \selectlanguage{english}(ruka = `hand'), for which the algorithm will try to generate paradigms. In our dataset we have inflection tables for the `hard', `soft', `iotated' and `mixed' groups of the 1st declination of nouns, taken from a Ukrainian grammar textbook \cite{hryshchenko1997sulm}: \selectlanguage{ukrainian}фабрика, робітниця, надія, площа \selectlanguage{english}(fabryka = `factory', robitnyts'a = `worker', nadija = `hope', ploshcha = `town square'): we use only inflection sets and morphological values from these tables (the stems in the inflection tables are only for illustration).

\begin{figure*}
	\includegraphics[width=\linewidth]{algorithm-illustration.jpg}
	\caption{Illustration of the algorithm.}
	\label{fig:alg_example}
\end{figure*}

In the first stage the algorithm tries every inflection in every table to split the current token ('ruka'). A possible split is found in the inflection table for the 1st declination of nouns, for the `hard' and `mixed' groups illustrated by examples `fabryka' and `ploshcha'. The split separates the stem \emph{'ruk'} and the inflection \emph{'a'}.

In the second stage, trying the split for the `hard' group, the word form hypotheses are generated from the inflection table: \emph{ruk+y, ruk+u, ruk+oju, ruk-o, ruk, ruk+amy, ruk+ax} and with the distortion model: [k] /\_[i] -> [ts] -- \emph{ruts+i}. For the split defined by the `mixed' group inflection table, in addition two incorrect word forms will be generated \emph{*ruk+eju, *ruk-e}, but the following three correct forms will not be generated: \emph{ruk-y, ruk-oju, ruk-o}. Therefore, two paradigms for the split \emph{ruk|a} will be competing with each other.

In the third stage, each of the competing paradigms will be verified against the corpus: in our example, for the `hard' group the following four hypothesised word forms are actually found: \emph{ruk, ruk+am, ruk+ax, ruk+y}, which, together with the original \emph{ruk+a}, correspond to 7 morphological feature combinations, since \emph{ruk+y} is ambiguous having three interpretations. While for the `mixed' group only three hypothesised forms will be confirmed, because \emph{ruk+y} has not been predicted by the `mixed' paradigm. As a result, the correct `hard' paradigm will be ranked higher, with 5 confirmed word form hypotheses vs. 4 confirmed hypotheses for the wrong `mixed' paradigm. When the corpus gets larger, more clues may differentiate such closely competing paradigms and more correct rankings may be produced.

In the fourth stage the top-ranking paradigm is confirmed and previously unseen word forms are generated, as well as possible part-of-speech code, lemma and all possible morphological feature combinations for both seen and unseen word forms, such as values for case, number, gender, i.e., the unseen word form \emph{ruk+u} will be generated with its morphological information \emph{ruk-u : lem=ruka; PoS=N.acc.sing}.

Note that for a single token it is not possible to clearly distinguish between a competing wrong paradigm and an alternative legitimate paradigm, which corresponds to an different reading of an ambiguous word form. For example, the word form \emph{pryklad|y} belongs both to \emph{PoS=N.nom.plur; lemma=pryklad (`example')} and to \emph{PoS=V.imper.pers2.sing; lemma=pryklasty/pryklad+u (`to attach')}. The limitation of the algorithm is that only one of these correct paradigms is confirmed for the given word form \emph{pryklady}, depending on how many hypothesised word forms are found in corpus for each of the verbal or nominal paradigms. However, the same paradigm is confirmed via different routes, i.e., through splitting other word forms belonging to the same paradigm, e.g., in Figure~\ref{fig:alg_example} the paradigm for \emph{lemma=ruka, PoS=N} will be also confirmed via splitting the corpus tokens \emph{ruk|am, ruk|amy, ruk|ax}. This gives the proposed algorithm an advantage, compared to the approach described in \cite{ahlberg2015paradigm} for the case of ambiguous paradigms: alternative readings will be confirmed by the tokens in corpus which are unique for each of the alternative paradigms, e.g., \emph{pryklad+ut' (`they will attach')} vs. \emph{pryklad+om (`with an example')}. Interestingly, ambiguous word forms are not discarded: when unambiguous tokens are split for each of the intersecting paradigms, the ambiguous tokens will count in both cases and confirm both of the correct paradigms. This will not happen for competing wrong paradigms: their wrong word form predictions (such as \emph{*ruk+eju} in the example above)  will simply not be found in corpus, so they will not initiate the process for the alternative paradigm.

For the purposes of this experiment we evaluate the coverage given by the algorithm without a distortion model, as such alternations are more typical for the older lexicon and may not occur in recently borrowed items, e.g. [portu] (port.Gen.sing) - port (port.Nom.sing): [o] -> [i] alternation does not take place, as the word was borrowed after the phonological law of the `open syllable' no longer worked in Ukrainian. However, in future work such model may be learnt from data or directly coded as explicit linguistic knowledge, and in this way older paradigms may and live stem alternations may also be covered.



\section{Evaluating algorithm with corpus coverage}
\label{sec:Evaluation}

The algorithm is used for extending the Ukrainian morphological lexicon from four corpora: news, wikipedia, law and fiction, as well as from a combined corpus that merges these four corpora. Table~\ref{lang-uk-corpus-generation} shows the number of word forms generated from each of the corpora presented in Table~\ref{lang-uk-corpus-description}.

\begin{table}[]
	\begin{center}
		\begin{tabular}{|l|rr|}
			\hline \textbf{Corpus} & \textbf{No of generated word forms} & \textbf{ } 
			\\ \hline
			dict\_uk & 5,690,688  &  \\
			\hline
			News & 3,292,591 &  \\
			Wikipedia & 3,765,774 &  \\
			Fiction & 958,233 &  \\
			Law & 1,788,288 &  \\
			\hline
			All corpora & 6,626,004 &  \\
			\hline
		\end{tabular}
	\end{center}
	\caption{\label{lang-uk-corpus-generation} Size of lexicon generated from corpora. }
\end{table}

We measure the coverage (in terms of lexical types) in four corpora and the merged corpus, with gradually filtering out lower frequency ranges. The rationale for this evaluation method is that it is usually harder for the lexicon to cover low-frequent items, so we test our lexicons on a range of tasks of varying difficulty.

The coverage of the existing static lexicon from the dict\_uk project developed by \cite{Rysin-Starko-2019} (the first entry in Table~\ref{lang-uk-corpus-generation}) is shown in Figure~\ref{fig:pcNoTypDictUK}.The horizontal axis indicates which frequency range has been filtered out. (Note the change of scale in the middle of the graph from 1 to 10 in one unit of length).

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-dict_uk.jpg}
	\caption{Percent of non-covered types (y) in dict\_uk baseline, with filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypDictUK}
\end{figure}

It can be seen from the figure that the 'dynamic' Wikipedia corpus that is being constantly updated and contains many Named Entities and specialised terminology that have the biggest problem with coverage: up to 80\% of its tokens are not covered, which goes down only to around 50\% if the frequency threshold is reduced to 10. At the same time a 'static' corpus of fiction texts is covered the best by the existing morphological lexicon.

We evaluate the effect of the proposed algorithm via measuring improvements in coverage of lexical types across the same frequency ranges for filtered out items. We use different corpora for the development and evaluation, so the following figures show the corpus coverage for these different combinations (lower lines indicate better results). Figure~\ref{fig:pcNoTypWikiN} and Figure~\ref{fig:pcNoTypWikiL} show coverage levels for the Wiki corpus with paradigms generated from the News and Law corpora respectively. Figure~\ref{fig:pcNoTypNewsW} and Figure~\ref{fig:pcNoTypNewsL} show coverage for the News corpus with paradigms developed from the Wiki and Law corpora. Finally, Figure~\ref{fig:pcNoTypLawW} and Figure~\ref{fig:pcNoTypLawN} show coverage for the Law corpus with paradigms developed from the Wiki and News corpora.

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-wikiN.jpg}
	\caption{Percent of non-covered types (y) in Wiki corpus, with dict\_uk (`wiki' line), with only our algorithm and paradigms generated from News corpus (`Par(w)' line), and with the two morphological lexicons combined (wiki+Par(w) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypWikiN}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-wikiL.jpg}
	\caption{Percent of non-covered types (y) in Wiki corpus, with dict\_uk (`wiki' line), with only our algorithm and paradigms generated from Law corpus (`Par(w)' line), and with the two morphological lexicons combined (wiki+Par(w) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypWikiL}
\end{figure}


\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-newsW.jpg}
	\caption{Percent of non-covered types (y) in News corpus, with dict\_uk (`news' line), with only our algorithm and paradigms generated from Wiki corpus (`Par(n)' line), and with the two morphological lexicons combined (news+Par(n) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypNewsW}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-newsL.jpg}
	\caption{Percent of non-covered types (y) in News corpus, with dict\_uk (`news' line), with only our algorithm and paradigms generated from Law corpus (`Par(n)' line), and with the two morphological lexicons combined (news+Par(n) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypNewsL}
\end{figure}


\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-lawW.jpg}
	\caption{Percent of non-covered types (y) in Law corpus, with dict\_uk (`law' line), with only our algorithm and paradigms generated from Wiki corpus (`Par(l)' line), and with the two morphological lexicons combined (law+Par(l) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypLawW}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-coverage-lawN.jpg}
	\caption{Percent of non-covered types (y) in Law corpus, with dict\_uk (`law' line), with only our algorithm and paradigms generated from News corpus (`Par(l)' line), and with the two morphological lexicons combined (law+Par(l) line); filtered out lower frequencies, up to (x).}
	\label{fig:pcNoTypLawN}
\end{figure}


It can be seen from the figures that for our algorithm the morpholigical and lexical diversity of the corpus are essential: the Law corpus has very little effect on the coverage of both News and Wikipedia corpora, while the News and Wikipedia consistently improve the coverage of all the corpora on which they are evaluated. This may be due to the small type/token ratio (i.e., small lexical diversity) of the Law corpus.

Finally, Figures~\ref{fig:pcDiffNoTypNews} and \ref{fig:pcDiffNoTypWiki} summarise improvement rates (i.e., the difference between the baseline and the proposed approach) for all the corpora using the News and Wiki corpora for generating paradigms.

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-improv-DiffNoTypNews.jpg}
	\caption{Improvement rates (y), of different corpora with the paradigms generated from New corpus; filtered out lower frequencies, up to (x).}
	\label{fig:pcDiffNoTypNews}
\end{figure}

\begin{figure}
	\includegraphics[width=\linewidth]{evaluation-improv-DiffNoTypWiki.jpg}
	\caption{Improvement rates (y), of different corpora with the paradigms generated from Wiki corpus; filtered out lower frequencies, up to (x).}
	\label{fig:pcDiffNoTypWiki}
\end{figure}


It can be seen from these figures that the coverage of Fiction and Law corpora is harder to improve, while News and Wiki corpora are most complementary, improving each other well. Also an interesting effect can be observed when a corpus is used to improve itself: the improvement rate peaks at the value of filtered frequencies up to 4, which may be interpreted as improvement in reliability of paradigm prediction for more frequent items, and as an indication of a possible threshold for the minimal number of slots for predicting Ukrainian paradigms.

The results indicate that the proposed approach gives consistent improvements in coverage with paradigms generated from a lexically diverse corpora with sufficient number of neologisms and new named entities. The highest improvement rates across different corpora has been achieved for paradigms generated from the Wiki corpus used to test the News corpus -- 16.3\% for lexical items with frequencies 10 and higher.

\section{Discussion}

Most lexical items covered with the proposed paradigm generation algorithm are Named Entities -- names of organisations, geographical places and person names, as well as technical terms, such as \selectlanguage{ukrainian}мінохоронздоров'я \selectlanguage{english} (`Ministry of Health') 
\selectlanguage{ukrainian}інтербізнесконсалтинг \selectlanguage{english} (`Internet business consulting'), \selectlanguage{ukrainian}кременчукм'ясо \selectlanguage{english} (`Meat of Kremenchuk' company), \selectlanguage{ukrainian}кривбасводопостачання \selectlanguage{english} (`Kryvbas Water Supply'), \selectlanguage{ukrainian}броваритепловодоенергія \selectlanguage{english} (`Brovary Heating, Water and Energy' company), \selectlanguage{ukrainian}могадішо \selectlanguage{english} (`Mogadishu')
\selectlanguage{ukrainian}озоноруйнуючих \selectlanguage{english}. (`ozone-destroying). 

However, the list also contains interesting political lexicon, such as \selectlanguage{ukrainian}йолка \selectlanguage{english} (`Christmas tree': the distorted ukrainized spelling of the mocked Russian word became the symbol of civil resistance during the Ukrainian revolution of dignity in 2013-2014) and \selectlanguage{ukrainian}проффесор \selectlanguage{english} (the distorted spelling of the word `professor', which was used for mocking the fugitive pro-Russian president, who held this title, but allegedly misspelt it on an official document). 

The appearance of this politically charged lexicon confirms the suggestion by \cite{karpilovska2013aktyvni} that lexical changes are driven by the social dynamics, especially at the times of major political developments in the society. However, even at the times of such political changes such lexicon is sill much less frequent and less changeable compared to Named Entities, which dominate the new lexicon.


\section{Conclusions and future work}

The proposed algorithm complements static linguistic resources and increases corpus coverage for new entities, such as neologisms and proper names. The highest improvements are achieved for genres that are expected to have many neologisms, specialised terminological lexicon and Named Entities, which are not well covered by the existing morphological lexicon: the Wikipedia and News. The advantage of the proposed approach is that it uses unlabelled corpora and small inflection tables for unsupervised induction of paradigms. However, in this stage it doesn't predict irregular paradigms. 

Future work will involve the development of distortion models to cover less regular cases and a systematic evaluation of the accuracy of paradigm prediction for different frequency ranges: while for more frequent items such prediction is highly reliable, there is a need to experimentally establish frequency and coverage thresholds for different error rates on this task. Another area of future research is the use of contextual features to verify predicted morphological properties. 

Improvement in coverage across corpora indicate that 

This document has been adapted from the instructions
for earlier ACL and NAACL proceedings,
including 
those for 
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
NAACL 2018 by Margaret Michell and Stephanie Lukin,
2017/2018 (NA)ACL bibtex suggestions from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, 
NAACL 2017 by Margaret Mitchell, 
ACL 2012 by Maggie Li and Michael White, 
those from ACL 2010 by Jing-Shing Chang and Philipp Koehn, 
those for ACL 2008 by JohannaD. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
those for ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
those for ACL 2002 by Eugene Charniak and Dekang Lin, 
and earlier ACL and EACL formats.
Those versions were written by several
people, including John Chen, Henry S. Thompson and Donald
Walker. Additional elements were taken from the formatting
instructions of the \emph{International Joint Conference on Artificial
  Intelligence} and the \emph{Conference on Computer Vision and
  Pattern Recognition}.

\section{Introduction}

The following instructions are directed to authors of papers submitted
to ACL 2019 or accepted for publication in its proceedings. All
authors are required to adhere to these specifications. Authors are
required to provide a Portable Document Format (PDF) version of their
papers. \textbf{The proceedings are designed for printing on A4
paper.}

\section{General Instructions}

Manuscripts must be in two-column format.  Exceptions to the
two-column format include the title, authors' names and complete
addresses, which must be centered at the top of the first page, and
any full-width figures or tables (see the guidelines in
Subsection~\ref{ssec:first}). \textbf{Type single-spaced.} 

 

Start all
pages directly under the top margin. See the guidelines later
regarding formatting the first page.  The manuscript should be
printed single-sided and its length
should not exceed the maximum page limit described in Section~\ref{sec:length}.
Pages are numbered for  initial submission. However, \textbf{do not number the pages in the camera-ready version}.

By uncommenting {\small\verb|\aclfinalcopy|} at the top of this 
 document, it will compile to produce an example of the camera-ready formatting; by leaving it commented out, the document will be anonymized for initial submission.  When you first create your submission on softconf, please fill in your submitted paper ID where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top.
 
The review process is double-blind, so do not include any author information (names, addresses) when submitting a paper for review.  
However, you should maintain space for names and addresses so that they will fit in the final (accepted) version.  The ACL 2019 \LaTeX\ style will create a titlebox space of 2.5in for you when {\small\verb|\aclfinalcopy|} is commented out.  

\subsection{The Ruler}
The ACL 2019 style defines a printed ruler which should be presented in the
version submitted for review.  The ruler is provided in order that
reviewers may comment on particular lines in the paper without
circumlocution.  If you are preparing a document without the provided
style files, please arrange for an equivalent ruler to
appear on the final output pages.  The presence or absence of the ruler
should not change the appearance of any other content on the page.  The
camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment the {\small\verb|\aclfinalcopy|} command in the document preamble.)  

Reviewers: note that the ruler measurements do not align well with
lines in the paper -- this turns out to be very difficult to do well
when the paper contains many figures and equations, and, when done,
looks ugly. In most cases one would expect that the approximate
location will be adequate, although you can also use fractional
references (\emph{e.g.}, the first paragraph on this page ends at mark $108.5$).

\subsection{Electronically-available resources}

ACL provides this description in \LaTeX2e{} (\texttt{\small acl2019.tex}) and PDF
format (\texttt{\small acl2019.pdf}), along with the \LaTeX2e{} style file used to
format it (\texttt{\small acl2019.sty}) and an ACL bibliography style (\texttt{\small acl\_natbib.bst})
and example bibliography (\texttt{\small acl2019.bib}).
These files are all available at
\texttt{\small http://acl2019.org/downloads/ acl2019-latex.zip}. 
 We
strongly recommend the use of these style files, which have been
appropriately tailored for the ACL 2019 proceedings.

\subsection{Format of Electronic Manuscript}
\label{sect:pdf}

For the production of the electronic manuscript you must use Adobe's
Portable Document Format (PDF). PDF files are usually produced from
\LaTeX\ using the \textit{pdflatex} command. If your version of
\LaTeX\ produces Postscript files, you can convert these into PDF
using \textit{ps2pdf} or \textit{dvipdf}. On Windows, you can also use
Adobe Distiller to generate PDF.

Please make sure that your PDF file includes all the necessary fonts
(especially tree diagrams, symbols, and fonts with Asian
characters). When you print or create the PDF file, there is usually
an option in your printer setup to include none, all or just
non-standard fonts.  Please make sure that you select the option of
including ALL the fonts. \textbf{Before sending it, test your PDF by
  printing it from a computer different from the one where it was
  created.} Moreover, some word processors may generate very large PDF
files, where each page is rendered as an image. Such images may
reproduce poorly. In this case, try alternative ways to obtain the
PDF. One way on some systems is to install a driver for a postscript
printer, send your document to the printer specifying ``Output to a
file'', then convert the file to PDF.

It is of utmost importance to specify the \textbf{A4 format} (21 cm
x 29.7 cm) when formatting the paper. When working with
\texttt{dvips}, for instance, one should specify \texttt{-t a4}.
Or using the command \verb|\special{papersize=210mm,297mm}| in the latex
preamble (directly below the \verb|\usepackage| commands). Then using 
\texttt{dvipdf} and/or \texttt{pdflatex} which would make it easier for some.

Print-outs of the PDF file on A4 paper should be identical to the
hardcopy version. If you cannot meet the above requirements about the
production of your electronic submission, please contact the
publication chairs as soon as possible.

\subsection{Layout}
\label{ssec:layout}

Format manuscripts two columns to a page, in the manner these
instructions are formatted. The exact dimensions for a page on A4
paper are:

\begin{itemize}
\item Left and right margins: 2.5 cm
\item Top margin: 2.5 cm
\item Bottom margin: 2.5 cm
\item Column width: 7.7 cm
\item Column height: 24.7 cm
\item Gap between columns: 0.6 cm
\end{itemize}

\noindent Papers should not be submitted on any other paper size.
 If you cannot meet the above requirements about the production of 
 your electronic submission, please contact the publication chairs 
 above as soon as possible.

\subsection{Fonts}

For reasons of uniformity, Adobe's \textbf{Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\begin{quote}
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\end{quote}
in the preamble. If Times Roman is unavailable, use \textbf{Computer
  Modern Roman} (\LaTeX2e{}'s default).  Note that the latter is about
  10\% less dense than Adobe's Times Roman font.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \textbf{Type of Text} & \textbf{Font Size} & \textbf{Style} \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
subsection titles & 11 pt & bold \\
document text & 11 pt  &\\
captions & 10 pt & \\
abstract text & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

\subsection{The First Page}
\label{ssec:first}

Center the title, author's name(s) and affiliation(s) across both
columns. Do not use footnotes for affiliations. Do not include the
paper ID number assigned during the submission process. Use the
two-column format only when you begin the abstract.

\textbf{Title}: Place the title centered at the top of the first page, in
a 15-point bold font. (For a complete guide to font sizes and styles,
see Table~\ref{font-table}) Long titles should be typed on two lines
without a blank line intervening. Approximately, put the title at 2.5
cm from the top of the page, followed by a blank line, then the
author's names(s), and the affiliation on the following line. Do not
use only initials for given names (middle initials are allowed). Do
not format surnames in all capitals (\emph{e.g.}, use ``Mitchell'' not
``MITCHELL'').  Do not format title and section headings in all
capitals as well except for proper names (such as ``BLEU'') that are
conventionally in all capitals.  The affiliation should contain the
author's complete address, and if possible, an electronic mail
address. Start the body of the first page 7.5 cm from the top of the
page.

The title, author names and addresses should be completely identical
to those entered to the electronical paper submission website in order
to maintain the consistency of author information among all
publications of the conference. If they are different, the publication
chairs may resolve the difference without consulting with you; so it
is in your own interest to double-check that the information is
consistent.

\textbf{Abstract}: Type the abstract at the beginning of the first
column. The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.6 cm on each side. Center the word \textbf{Abstract} in a 12 point bold
font above the body of the abstract. The abstract should be a concise
summary of the general thesis and conclusions of the paper. It should
be no longer than 200 words. The abstract text should be in 10 point font.

\textbf{Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in the present document. Do not include page numbers.

\textbf{Indent}: Indent when starting a new paragraph, about 0.4 cm. Use 11 points for text and subsection headings, 12 points for section headings and 15 points for the title. 


\begin{table}
\centering
\small
\begin{tabular}{cc}
\begin{tabular}{|l|l|}
\hline
\textbf{Command} & \textbf{Output}\\\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular} & 
\begin{tabular}{|l|l|}
\hline
\textbf{Command} & \textbf{Output}\\\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\\hline
\end{tabular}
\end{tabular}
\caption{Example commands for accented characters, to be used in, \emph{e.g.}, \BibTeX\ names.}\label{tab:accents}
\end{table}

\subsection{Sections}

\textbf{Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals.
Do not number subsubsections.

\begin{table*}[t!]
\centering
\begin{tabular}{lll}
  output & natbib & previous ACL style files\\
  \hline
  \citep{Gusfield:97} & \verb|\citep| & \verb|\cite| \\
  \citet{Gusfield:97} & \verb|\citet| & \verb|\newcite| \\
  \citeyearpar{Gusfield:97} & \verb|\citeyearpar| & \verb|\shortcite| \\
\end{tabular}
\caption{Citation commands supported by the style file.
  The citation style is based on the natbib package and
  supports all natbib citation commands.
  It also supports commands defined in previous ACL style files
  for compatibility.
  }
\end{table*}

\textbf{Citations}: Citations within the text appear in parentheses
as~\cite{Gusfield:97} or, if the author's name appears in the text
itself, as Gusfield~\shortcite{Gusfield:97}.
Using the provided \LaTeX\ style, the former is accomplished using
{\small\verb|\cite|} and the latter with {\small\verb|\shortcite|} or {\small\verb|\newcite|}. Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}; this is accomplished with the provided style using commas within the {\small\verb|\cite|} command, \emph{e.g.}, {\small\verb|\cite{Gusfield:97,Aho:72}|}. Append lowercase letters to the year in cases of ambiguities.  
 Treat double authors as
in~\cite{Aho:72}, but write as in~\cite{Chandra:81} when more than two
authors are involved. Collapse multiple citations as
in~\cite{Gusfield:97,Aho:72}. Also refrain from using full citations
as sentence constituents.

We suggest that instead of
\begin{quote}
  ``\cite{Gusfield:97} showed that ...''
\end{quote}
you use
\begin{quote}
``Gusfield \shortcite{Gusfield:97}   showed that ...''
\end{quote}

If you are using the provided \LaTeX{} and Bib\TeX{} style files, you
can use the command \verb|\citet| (cite in text)
to get ``author (year)'' citations.

You can use the command \verb|\citealp| (alternative cite without 
parentheses) to get ``author year'' citations (which is useful for 
using citations within parentheses, as in \citealp{Gusfield:97}).

If the Bib\TeX{} file contains DOI fields, the paper
title in the references section will appear as a hyperlink
to the DOI, using the hyperref \LaTeX{} package.
To disable the hyperref package, load the style file
with the \verb|nohyperref| option: \\{\small
\verb|\usepackage[nohyperref]{acl2019}|}

\textbf{Compilation Issues}: Some of you might encounter the following error during compilation: 

``{\em \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.}''

This happens when \verb|pdflatex| is used and a citation splits across a page boundary. To fix this, the style file contains a patch consisting of the following two lines: (1) \verb|\RequirePackage{etoolbox}| (line 454 in \texttt{acl2019.sty}), and (2) A long line below (line 455 in \texttt{acl2019.sty}).

If you still encounter compilation issues even with the patch enabled, disable the patch by commenting the two lines, and then disable the \verb|hyperref| package (see above), recompile and see the problematic citation.
Next rewrite that sentence containing the citation. (See, {\em e.g.}, {\small\tt http://tug.org/errors.html})

\textbf{Digital Object Identifiers}:  As part of our work to make ACL
materials more widely used and cited outside of our discipline, ACL
has registered as a CrossRef member, as a registrant of Digital Object
Identifiers (DOIs), the standard for registering permanent URNs for
referencing scholarly materials.  As of 2017, we are requiring all
camera-ready references to contain the appropriate DOIs (or as a
second resort, the hyperlinked ACL Anthology Identifier) to all cited
works.  Thus, please ensure that you use Bib\TeX\ records that contain
DOI or URLs for any of the ACL materials that you reference.
Appropriate records should be found for most materials in the current
ACL Anthology at \url{http://aclanthology.info/}.

As examples, we cite \cite{P16-1001} to show you how papers with a DOI
will appear in the bibliography.  We cite \cite{C14-1001} to show how
papers without a DOI but with an ACL Anthology Identifier will appear
in the bibliography.  

As reviewing will be double-blind, the submitted version of the papers
should not include the authors' names and affiliations. Furthermore,
self-references that reveal the author's identity, \emph{e.g.},
\begin{quote}
``We previously showed \cite{Gusfield:97} ...''  
\end{quote}
should be avoided. Instead, use citations such as 
\begin{quote}
``\citeauthor{Gusfield:97} \shortcite{Gusfield:97}
previously showed ... ''
\end{quote}

Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper. ACL 2019 reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form. 


\textbf{Please do not use anonymous citations} and do not include
 when submitting your papers. Papers that do not
conform to these requirements may be rejected without review.

\textbf{References}: Gather the full set of references together under
the heading \textbf{References}; place the section before any Appendices. 
Arrange the references alphabetically
by first author, rather than by order of occurrence in the text.
By using a .bib file, as in this template, this will be automatically 
handled for you. See the \verb|\bibliography| commands near the end for more.

Provide as complete a citation as possible, using a consistent format,
such as the one for \emph{Computational Linguistics\/} or the one in the 
\emph{Publication Manual of the American 
Psychological Association\/}~\cite{APA:83}. Use of full names for
authors rather than initials is preferred. A list of abbreviations
for common computer science journals can be found in the ACM 
\emph{Computing Reviews\/}~\cite{ACM:83}.

The \LaTeX{} and Bib\TeX{} style files provided roughly fit the
American Psychological Association format, allowing regular citations, 
short citations and multiple citations as described above.  

\begin{itemize}
\item Example citing an arxiv paper: \cite{rasooli-tetrault-2015}. 
\item Example article in journal citation: \cite{Ando2005}.
\item Example article in proceedings, with location: \cite{borsch2011}.
\item Example article in proceedings, without location: \cite{andrew2007scalable}.
\end{itemize}
See corresponding .bib file for further details.

Submissions should accurately reference prior and related work, including code and data. If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced. If multiple versions of a piece of prior work exist, the one used by the authors should be referenced. Authors should not rely on automated citation indices to provide accurate references for prior and related work.

\textbf{Appendices}: Appendices, if any, directly follow the text and the
references (but see above).  Letter them in sequence and provide an
informative title: \textbf{Appendix A. Title of Appendix}.

\subsection{Footnotes}

\textbf{Footnotes}: Put footnotes at the bottom of the page and use 9
point font. They may be numbered or referred to by asterisks or other
symbols.\footnote{This is how a footnote should appear.} Footnotes
should be separated from the text by a line.\footnote{Note the line
separating the footnotes from the text.}

\subsection{Graphics}

\textbf{Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns.  Color
illustrations are discouraged, unless you have verified that  
they will be understandable when printed in black ink.

\textbf{Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and 
tables below the body, using 10 point text. Captions should be placed below illustrations. Captions that are one line are centered (see Table~\ref{font-table}). Captions longer than one line are left-aligned (see Table~\ref{tab:accents}). Do not overwrite the default caption sizes. The acl2019.sty file is compatible with the caption and subcaption packages; do not add optional arguments.


\subsection{Accessibility}
\label{ssec:accessibility}

In an effort to accommodate people who are color-blind (as well as those printing
to paper), grayscale readability for all accepted papers will be
encouraged.  Color is not forbidden, but authors should ensure that
tables and figures do not rely solely on color to convey critical
distinctions. A simple criterion: All curves and points in your figures should be clearly distinguishable without color.

% Min: no longer used as of ACL 2018, following ACL exec's decision to
% remove this extra workflow that was not executed much.
% BEGIN: remove
%% \section{XML conversion and supported \LaTeX\ packages}

%% Following ACL 2014 we will also we will attempt to automatically convert 
%% your \LaTeX\ source files to publish papers in machine-readable 
%% XML with semantic markup in the ACL Anthology, in addition to the 
%% traditional PDF format.  This will allow us to create, over the next 
%% few years, a growing corpus of scientific text for our own future research, 
%% and picks up on recent initiatives on converting ACL papers from earlier 
%% years to XML. 

%% We encourage you to submit a ZIP file of your \LaTeX\ sources along
%% with the camera-ready version of your paper. We will then convert them
%% to XML automatically, using the LaTeXML tool
%% (\url{http://dlmf.nist.gov/LaTeXML}). LaTeXML has \emph{bindings} for
%% a number of \LaTeX\ packages, including the ACL 2018 stylefile. These
%% bindings allow LaTeXML to render the commands from these packages
%% correctly in XML. For best results, we encourage you to use the
%% packages that are officially supported by LaTeXML, listed at
%% \url{http://dlmf.nist.gov/LaTeXML/manual/included.bindings}
% END: remove

\section{Translation of non-English Terms}

It is also advised to supplement non-English characters and terms
with appropriate transliterations and/or translations
since not all readers understand all such characters and terms.
Inline transliteration or translation can be represented in
the order of: original-form transliteration ``translation''.

\section{Length of Submission}
\label{sec:length}

The ACL 2019 main conference accepts submissions of long papers and
short papers.
 Long papers may consist of up to eight (8) pages of
content plus unlimited pages for references. Upon acceptance, final
versions of long papers will be given one additional page -- up to nine (9)
pages of content plus unlimited pages for references -- so that reviewers' comments
can be taken into account. Short papers may consist of up to four (4)
pages of content, plus unlimited pages for references. Upon
acceptance, short papers will be given five (5) pages in the
proceedings and unlimited pages for references. 
For both long and short papers, all illustrations and tables that are part
of the main text must be accommodated within these page limits, observing
the formatting instructions given in the present document. Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.

ACL 2019 does encourage the submission of additional material that is relevant to the reviewers but not an integral part of the paper. There are two such types of material: appendices, which can be read, and non-readable supplementary materials, often data or code.  Do not include this additional material in the same document as your main paper. Additional material must be submitted as one or more separate files, and must adhere to the same anonymity guidelines as the main paper. The paper must be self-contained: it is optional for reviewers to look at the supplementary material. Papers should not refer, for further detail, to documents, code or data resources that are not available to the reviewers. Refer to Appendix~\ref{sec:appendix} and Appendix~\ref{sec:supplemental} for further information. 

Workshop chairs may have different rules for allowed length and
whether supplemental material is welcome. As always, the respective
call for papers is the authoritative source.

\section*{Acknowledgments}

The acknowledgments should go immediately before the references.  Do
not number the acknowledgments section. Do not include this section
when submitting your paper for review. \\

\noindent \textbf{Preparing References:} \\
Include your own bib file like this:
\verb|\bibliographystyle{acl_natbib}|
\verb|\bibliography{acl2019}| 

where \verb|acl2019| corresponds to a acl2019.bib file.
\bibliography{acl2019}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}
Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. 
Appendices should be \textbf{uploaded as supplementary material} when submitting the paper for review. Upon acceptance, the appendices come after the references, as shown here. Use
\verb|\appendix| before any appendix section to switch the section
numbering over to letters.


\section{Supplemental Material}
\label{sec:supplemental}
Submissions may include non-readable supplementary material used in the work and described in the paper. Any accompanying software and/or data should include licenses and documentation of research review as appropriate. Supplementary material may report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper. Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. 

Nonetheless, supplementary material should be supplementary (rather
than central) to the paper. \textbf{Submissions that misuse the supplementary 
material may be rejected without review.}
Supplementary material may include explanations or details
of proofs or derivations that do not fit into the paper, lists of
features or feature templates, sample inputs and outputs for a system,
pseudo-code or source code, and data. (Source code and data should
be separate uploads, rather than part of the paper).

The paper should not rely on the supplementary material: while the paper
may refer to and cite the supplementary material and the supplementary material will be available to the
reviewers, they will not be asked to review the
supplementary material.


\end{document}
